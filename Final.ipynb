{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lizZPUGSgiP5",
        "outputId": "0c52bdee-d614-426d-ae7c-0bda061f2fde"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jbg3uDeugSzH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils \n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Winter/Public.csv', low_memory=False,encoding='utf-8') ### 데이터셋 불러오기\n",
        "df = df.iloc[:, [4,6]] ### 분류번호와 제목만 가져옵니다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(subset = ['title'], inplace=True)  #제목에 겹치는게 많아 이럴 경우 중복 데이터를 삭제합니다.\n",
        "\n",
        "df['title'] = df['title'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\") #영어 분류는 제외하기로 하였기에 영어를 공백으로 제거\n",
        "\n",
        "df['title'] = df['title'].str.replace('^ +', \"\")#공백과 띄어쓰기만 있는 셀들을 확인\n",
        "\n",
        "df['class_no']= df['class_no'].str.replace(\"[^0-9]\", \"\") #라벨지에 가끔 숫자 대신 글자가 들어가 있는 경우를 확인 이를 제거\n",
        "\n",
        "df['class_no'] = df['class_no'].str.replace('^ +', \"\")#공백과 띄어쓰기만 있는 셀들을 확인\n",
        "\n",
        "df.replace('', np.nan, inplace=True)#이를 drop하기 위하여 None으로 변경합니다.\n",
        "\n",
        "df = df.dropna(how = 'any') #none은 전부 드랍\n",
        "\n",
        "df = df.reset_index(drop = True) #이 경우 셀들의 배열이 맞지않아 배열을 다시 하여 줍니다.\n",
        "\n",
        "X = df.iloc[:, 1] #제목만 따로 분리\n",
        "y = df.iloc[:, 0] #라벨만 따로 분리\n",
        "y = y.str[:1]  #라벨의 앞자리 즉 분류 자리수만 가져와 확인합니다.\n",
        "\n",
        "y = y.astype(int)\n",
        "y= y.iloc[:]\n",
        "X= np.array(X)\n",
        "y= np.array(y)\n",
        "y = np_utils.to_categorical(y)\n",
        "  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=1004)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmG1BT4AnLSI",
        "outputId": "53db23ae-bec2-4fa2-ee8f-35001b515638"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train) #X_train을 리스트의 형태로 변경\n",
        "word_to_index = tokenizer.word_index\n",
        "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
        "print(X_train_encoded[:5])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CIM1z-xPj9Td",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e52ceaa-05c6-48f4-9d61-0835ff6ca9ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[49053, 80020], [80021, 80022], [526, 1243], [220, 35521, 11308], [17052, 19533, 1190, 458, 1487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 2\n",
        "total_cnt = len(word_to_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "vocab_size = len(word_to_index) + 1\n",
        "print('제목의 최대 길이 : %d' % max(len(sample) for sample in X_train_encoded))\n",
        "print('단어의 갯수 :' , total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합(vocabulary)에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCTrF1enawFU",
        "outputId": "bd1e1a5d-f3de-45ed-de0d-1be219593a9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "제목의 최대 길이 : 50\n",
            "단어의 갯수 : 218947\n",
            "등장 빈도가 1번 이하인 희귀 단어의 수: 138928\n",
            "단어 집합(vocabulary)에서 희귀 단어의 비율: 63.452799079229216\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 13.247362027305615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen = 50)\n",
        "print(X_train_padded[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AF7HxAzwgsN",
        "outputId": "43ee3be8-2f3f-4348-be65-06e39d4d1da3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "  49053 80020]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "  80021 80022]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "    526  1243]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0   220\n",
            "  35521 11308]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0 17052 19533  1190\n",
            "    458  1487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "hidden_units = 64\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, 16, input_shape=(None,)))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(keras.layers.Dense(10, activation='softmax')) \n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "result = model.fit(X_train_padded, y_train, epochs=3, batch_size=64)\n",
        "print(\"training complete\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q5lnWF8xCPE",
        "outputId": "d113ef61-156d-4036-ccad-3ad9d4f46bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 16)          3503168   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 64)                20736     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,524,554\n",
            "Trainable params: 3,524,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/3\n",
            " 254/5451 [>.............................] - ETA: 8:53 - loss: 0.3083 - accuracy: 0.3636"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_encoded = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen = 40)\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(X_test_padded, y_test)[1]))"
      ],
      "metadata": {
        "id": "FtKZDOnyLgC9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}